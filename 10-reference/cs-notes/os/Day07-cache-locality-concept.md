# Day07 – 캐시 & 지역성(Locality) (Concept)

## 1. 이 개념을 왜 배워야 하는가? (문제의식)

서버가 빠르게 동작하는 이유는 CPU가 빠른 것 때문만은 아니다.  
**실제 성능의 핵심은 ‘데이터를 얼마나 빨리 가져올 수 있느냐’**이다.  
현대 컴퓨터에서 CPU는 너무 빠르고, 메모리는 상대적으로 느리기 때문에  
두 장치 사이의 속도 차이를 줄이기 위한 핵심 기술이 바로 **캐시(Cache)**다.

이를 이해하지 못하면 다음과 같은 문제를 분석하기 어렵다:

- 특정 연산은 빠른데 배열 연산은 갑자기 느려지는 이유  
- JVM 성능 분석 시 “캐시 미스(cache miss)”가 왜 문제인지  
- CPU가 놀고 있는데 응답이 느려지는 상황  
- 데이터 배치 방식에 따른 성능 차이를 해석할 수 없음  
- 반복문 하나만 바꿔도 성능이 크게 좋아지는 원리를 설명할 수 없음  

반대로 캐시와 지역성을 이해하면:
- 메모리 접근 성능 구조를 정확히 이해할 수 있고  
- CPU 성능을 최대한 끌어내는 데이터 구조 설계가 가능하며  
- GC·배열·컬렉션 성능 이슈를 원리적으로 분석할 수 있다  

즉, 캐시/지역성은 **백엔드 개발자가 시스템 레벨 성능을 이해하는 핵심 개념**이다.

---

## 2. 비유로 먼저 이해하기

### 📌 비유 1: “책상 위 vs 책장”
- 자주 보는 책은 책상 위(= 캐시)에 올려놓는다.  
- 잘 쓰지 않는 책은 책장(= 메모리)에 둔다.  
- 매번 책장에서 꺼내오면 느리다.  
- 책상 공간은 작기 때문에 **어떤 책을 올려놓을지 전략이 필요**하다.

---

### 📌 비유 2: “냉장고 앞 선반”
- 요리를 할 때 가장 자주 쓰는 재료를 앞 선반에 둔다.  
- 안 쓰면 다시 뒤쪽으로 밀림.  
- CPU도 “가장 최근에 사용한 데이터가 앞으로 오도록” 유지한다.  
  → 시간 지역성(Time Locality)

---

### 📌 비유 3: “마트에서 장보기”
- 같은 종류의 물건은 같은 진열대에 있다.  
- CPU도 “서로 가까운 메모리 주소에 있는 데이터는 한꺼번에 가져온다.”  
  → 공간 지역성(Spatial Locality)

---

## 3. 공식 개념 정의

### ✔ 캐시(Cache)
CPU와 메모리 사이의 속도 차이를 줄이기 위해  
**최근 사용하거나, 곧 사용할 가능성이 높은 데이터**를 미리 저장해 두는 작은 고속 메모리.

특징:
- 매우 빠르지만 매우 작다  
- 계층 구조(L1, L2, L3)로 되어 있다  
- 캐시 적중(cache hit) 여부가 성능을 결정한다  

---

### ✔ 지역성(Locality)
프로그램은 일반적으로 “특정 데이터와 코드를 반복적으로 접근하는 경향”이 있다.  
이 성질을 이용한 것이 캐시의 핵심 전략이다.

지역성은 크게 두 가지다:

1) **시간 지역성(Time Locality)**  
   - 최근에 사용한 데이터는 다시 사용할 가능성이 높다.  
   - 예: 반복문에서 변수 i 반복 접근  

2) **공간 지역성(Spatial Locality)**  
   - 어떤 주소를 접근하면 그 주변 주소도 사용할 가능성이 높다.  
   - 예: 배열 순차 접근  

---

## 4. 캐시 계층 구조 (L1 / L2 / L3)

CPU 캐시는 보통 다음과 같이 계층적이다:

1. **L1 캐시**  
   - 가장 빠르고 가장 작다  
   - 보통 32~64KB 수준  
   - 명령어 캐시(I-cache) / 데이터 캐시(D-cache) 분리되어 있음

2. **L2 캐시**  
   - L1보다 조금 느리고 크다  
   - 256KB~1MB 정도  

3. **L3 캐시**  
   - 코어 전체가 공유  
   - 몇 MB ~ 수십 MB  

캐시에서 찾지 못하면 최종적으로 **메인 메모리**에서 데이터를 가져오는데,  
이때 지연(latency)은 수십~수백 배 이상 차이가 난다.

→ **따라서 캐시 히트율이 전체 성능을 좌우한다.**

---

## 5. 캐시 적중(hit) vs 캐시 미스(miss)

### ✔ 캐시 히트 (Cache Hit)
필요한 데이터가 캐시에 이미 있다.  
→ 매우 빠른 수행 (수 ns 단위)

### ✔ 캐시 미스 (Cache Miss)
캐시에 없어서 메모리에서 가져와야 함.  
→ 지연이 매우 큼 (수십~수백 ns)

미스가 많아지면:
- CPU는 idle 상태가 증가  
- 스레드는 바쁘지 않은데 서버는 느려짐  
- 캐시 친화적인 코드 설계가 매우 중요해짐

---

## 6. 캐시 라인(Cache Line)

캐시는 데이터를 “조각 단위”로 가져온다.  
이 조각을 **캐시 라인(cache line)** 이라고 하며 보통 64바이트.

즉, 어떤 메모리 주소를 접근하면:
- 해당 주소 주변 64바이트를 한꺼번에 가져온다  
- 이 메커니즘 때문에 **공간 지역성**이 중요하다

배열을 순차적으로 접근하면 빠른 이유도 이것 때문이다.

---

## 7. 지역성과 성능의 관계

### ✔ 시간 지역성 예시
for (int i = 0; i < 1000000; i++) total += arr[i];

- total 변수는 계속 사용됨 → 시간 지역성 높음  
- 캐시 히트가 반복적으로 발생  

---

### ✔ 공간 지역성 예시
arr[i], arr[i+1], arr[i+2] ...

- 연속된 주소는 같은 캐시 라인으로 묶여 있기 때문에  
- 배열 순회가 매우 빠르다  

반대로 링크드 리스트 탐색은 느린 이유:
- 노드가 메모리 여기저기에 흩어져 있어  
- 공간 지역성이 낮기 때문

---

## 8. 실제 컴퓨터 내부에서는 어떤 일이 일어나는가?

- CPU는 먼저 L1 → L2 → L3 캐시를 차례로 탐색  
- 어디에도 없으면 메모리 접근 (수십~수백 배 느림)  
- 캐시가 꽉 차면 **교체 정책(예: LRU)** 에 따라 오래 안 쓴 데이터를 내보냄  
- 반복문·배열 순회는 캐시 친화적으로 동작  
- 불규칙한 메모리 접근(해시맵, 링크드 리스트)은 캐시 미스 유발  
- GC는 메모리의 연속성을 깨뜨리기 때문에 캐시 성능에 영향을 줄 수 있음  

---

## 9. 실무에서 왜 중요한가?

- 대량 데이터 처리 시 배열 구조 선택이 더 빠른 이유  
- GC 동작 시 CPU 캐시 비효율이 발생하는 원리 이해  
- JVM 최적화 시 “객체 배치(객체가 얼마나 흩어져 있는지)”를 고려  
- DB 캐시 / Redis 캐시 개념을 더 깊게 이해 가능  
- 서버가 CPU는 충분한데 느릴 때 캐시 미스가 원인일 수 있음  

특히 JVM 기반 시스템에서 **배열, 컬렉션, 반복문 성능 차이**는  
대부분 캐시/지역성의 차이로 설명된다.

---

## 10. 핵심 요약 10줄

1. 캐시는 CPU와 메모리 속도 차이를 줄이는 고속 메모리다.  
2. 캐시는 매우 빠르지만 매우 작다.  
3. 캐시 계층(L1~L3)은 속도와 크기의 균형 구조다.  
4. 캐시 라인(64B)은 메모리를 가져오는 기본 단위다.  
5. 지역성은 시간·공간 두 가지가 있다.  
6. 시간 지역성: 최근 사용한 데이터는 다시 사용될 가능성이 높다.  
7. 공간 지역성: 가까운 주소가 함께 사용될 가능성이 높다.  
8. 캐시 히트율이 시스템 전체 성능을 결정한다.  
9. 배열 순회가 빠른 이유는 캐시 라인에 맞는 접근 패턴 때문이다.  
10. 불규칙 접근은 캐시 미스를 증가시키므로 성능 저하를 일으킨다.